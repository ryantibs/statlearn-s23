\documentclass{article}

\def\ParSkip{} 
\input{../../common/ryantibs}

\title{Homework 2 \\ \smallskip
\large Advanced Topics in Statistical Learning, Spring 2023 \\ \smallskip
Due Friday March 3 at 5pm}
\date{}

\begin{document}
\maketitle
\RaggedRight
\vspace{-50pt}

\section{Properties of RKHS regression [16 points]}

In this exercise, we will work out some facts about RKHS regression. 

\begin{enumerate}[label=(\alph*)]
\item First, let $k : \cX \times \cX \to \R$ be an arbitrary kernel. Recall that
  this means there exists a feature map $\phi : \cX \to \cH$, and a Hilbert
  space $\cH$ with inner product $\langle \,\cdot\,, \,\cdot\, \rangle_\cH$,
  such that for any $x,y \in \cX$,    
  \[
  k(x,y) = \langle \phi(x), \phi(y) \rangle_\cH.
  \]
  Prove that $k$ is positive semidefinite, meaning, it is symmetric, and for any 
  $n \geq 1$ and $x_1,\dots,x_n \in \cX$, if we define a matrix $K \in \R^{n
    \times n}$ to have entries $K_{ij} = k(x_i, x_j)$, then  
  \marginpar{\small [3 pts]}
  \[
  a^\T K a \geq 0, \quad \text{for all $a \in \R^n$}.
  \]

  Hint: express $a^\T K a$ in terms of the feature map $\phi$.

\item Henceforth, suppose that $k$ is the reproducing kernel for $\cH$ (and
  hence $\cH$ is an RKHS). Recall that this means the following two properties
  are satisfied: 
  \begin{enumerate}
  \item for any $x \in \cX$, the function $k(\cdot, x)$ is an element of $\cH$;   
  \item for any function $f \in \cH$ and $x \in \cX$, it holds that $\langle f,
    k(\cdot, x) \rangle_\cH = f(x)$. 
  \end{enumerate}
  Let $f$ be a function of the form 
  \[  
  f(x) = \sum_{i=1}^n \beta_i k(x, x_i),
  \]
  for coefficients $\beta_1,\dots,\beta_n \in \R$. Show that 
  \marginpar{\small [2 pts]}
  \[
  \|f\|_\cH^2 = \sum_{i,j=1}^n \beta_i \beta_j k(x_i,x_j).
  \]

\item Let $h$ be any function (in $\cH$) that is orthogonal (with respect to
  $\langle \,\cdot\,, \,\cdot\, \rangle_\cH$) to the linear space of functions
  of the form in part (b). Prove that 
  \marginpar{\small [3 pts]}
  \[
  h(x_i) = 0, \quad i=1,\dots,n,
  \]
  and
  \[
  \|f + h\|_\cH \geq \|f\|_\cH, \quad \text{with equality iff $h=0$}. 
  \]

\item Argue that for any $\lambda > 0$, the infinite-dimensional RKHS
  optimization problem 
  \[
  \minimize_f \; \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \|f\|_\cH^2
  \]
  (where the minimization is implicitly over $f \in \cH$) has a unique solution
  of the form in part (b), and we can rewrite it as
  \marginpar{\small [2 pts]}
  \[
  \minimize_\beta \; \|Y - K\beta\|_2^2 + \lambda \beta^\T K \beta. 
  \]
  for the matrix $K \in \R^{n \times n}$ with entries $K_{ij} = k(x_i, x_j)$. 

  Hint: let $g = f+h$ and use the results in part (c) to argue that $g$ has a
  larger criterion value, unless $h=0$. Use part (b) to complete the reduction
  to finite-dimensional form. 

\item Finally, we establish a cool fact about leave-one-out cross-validation
  (LOOCV) in RKHS regression problems. Recall that in general, the LOOCV error
  of an estimator \smash{$\hf$} is defined as
  \[
  \mathrm{CV}(\hf) = \frac{1}{n} \sum_{i=1}^n (y_i - \hf^{-i}(x_i))^2,
  \]
  where \smash{$\hf^{-i}$} is the estimator trained on all but the $i\th$ pair 
  $(x_i,y_i)$. Prove the following shortcut formula for LOOCV with an RKHS
  regression estimator \smash{$\hf$}:  
  \marginpar{\small [6 pts]}
  \[
  \frac{1}{n} \sum_{i=1}^n (y_i - \hf^{-i}(x_i))^2 = 
  \frac{1}{n} \sum_{i=1}^n \bigg(\frac{y_i - \hf(x_i)}{1-S_{ii}}\bigg)^2,
  \]
  where $S=K(K + \lambda I)^{-1}$ is the smoother matrix for the RKHS estimator 
  (so that the vector of fitted values is given by \smash{$\hat{Y} = SY$}). 

  Hint: prove that for each $i$,
  \[
  \hf^{-i}(x_i) = \frac{1}{1-S_{ii}} [\hf(x_i) - S_{ii}y_i].
  \]
  The desired result will follow by rearranging, squaring both sides, and
  summing over $i=1,\dots,n$. There are different ways to establish the above
  display; one nice way is as follows. Consider solving the RKHS regression
  problem on 
  \[
    (x_1,y_1), \dots, (x_{i-1},y_{i-1}), (x_{i+1},y_{i+1}), \dots, (x_n,y_n), 
  \]
  and consider solving it on 
  \[
    (x_1,y_1), \dots, (x_{i-1},y_{i-1}), (x_i, \tilde{y}_i), (x_{i+1},y_{i+1}),
    \dots, (x_n,y_n), 
  \]
  where \smash{$\tilde{y}_i = \hf^{-i}(x_i)$}. Argue that these should produce
  the same solutions. Derive the zero gradient condition for optimality
  (differentiate the criterion and set it equal to zero) for each problem, and
  use these to solve for \smash{$\tilde{y}_i = \hf^{-i}(x_i)$}. 
\end{enumerate}

\section{Sub-Gaussian maximal inequalities [12 points]}

In this exercise, we will derive tail bounds on maxima of sub-Gaussian random
variables $X_i$, $i=1,\dots,n$. Suppose each $X_i$ has mean zero and variance
proxy $\sigma^2$. (We assume nothing about their dependence structure.)

\begin{enumerate}[label=(\alph*)]
\item Prove that for any $\lambda \in \R$,
  \marginpar{\small [3 pts]}
  \[
  \exp \bigg( \lambda \E\Big[ \max_{i=1,\dots,n} \, X_i \Big] \bigg) \leq n
  e^{\sigma^2 \lambda^2/2}.   
  \]

\item Rearrange the result in part (a), then choose a suitable value of
  $\lambda$ to show that     
  \marginpar{\small [2 pts]}
  \[
  \E \Big[ \max_{i=1,\dots,n} \, X_i \Big] \leq \sigma \sqrt{2 \log n}. 
  \]

\item Prove that for any $\lambda \in \R$,
  \marginpar{\small [2 pts]}
  \[
  \P \Big(\max_{i=1,\dots,n} \, X_i \geq \lambda \Big) \leq ne^{-\lambda^2 /
    (2\sigma^2)}.  
  \]

  Hint: use the fact that \smash{$\P(X_i \geq \lambda) \leq e^{-\lambda^2 / 
    (2\sigma^2)}$}, for any $\lambda \in \R$, which you can view as a
  consequence of the tail bound for sub-Gaussian averages when $n=1$. 

\item Reparametrize the result in part (c) to show that for any $t>0$,
  \marginpar{\small [1 pt]}
  \[
  \P \Big(\max_{i=1,\dots,n} \, X_i \geq \sigma \sqrt{2 (\log n + t)} \Big) 
  \leq e^{-t}.   
  \]

\item Now, we turn the question of the role of dependence: do correlations 
  between $X_i$, $i=1,\dots,n$ make their maximum stochastically larger or
  smaller? Conduct (and include the results of) a small simulation in order to
  inform your answer. 
  \marginpar{\small [4 pts]}
 \end{enumerate}

\section{Risk analysis for the constrained lasso [12 points]}

\def\hSigma{\hat\Sigma}

This exercise explores simple in-sample and out-of-sample risk bounds for the
lasso. Assume that we observe i.i.d.\ $(x_i,y_i) \in [-M,M]^d \times \R$,
$i=1,\dots,n$, where each   
\[
y_i = x_i^\T \beta_0 + \epsilon_i,
\]
and each $\epsilon_i$ is sub-Gaussian with mean zero and variance proxy
$\sigma^2$. Consider the constrained-form lasso estimator \smash{$\hbeta$}, 
which solves  
\[
\minimize_\beta \; \|Y - X\beta\|_2^2 \;\; \st \;\; \|\beta\|_1 \leq t,    
\]
where $Y = (y_1,\dots,y_n) \in \R^n$ is the response vector, $X \in \R^{n \times
  d}$ is the predictor matrix (whose $i\th$ row is $x_i \in \R^d$), and $t \geq
0$ is a tuning parameter.

\begin{enumerate}[label=(\alph*)]
\item Prove that the lasso estimator, with $t=\|\beta_0\|_1$, has in-sample risk
  satisfying 
  \marginpar{\small [3 pts]}
  \[
  \frac{1}{n} \E \|X\hbeta - X\beta_0\|_2^2 \leq 4M \sigma \|\beta_0\|_1
  \sqrt{\frac{2 \log(2d)}{n}},  
  \]
  where the expectation is taken over the training data $(x_i,y_i)$,
  $i=1,\dots,n$. 

  Hint: follow the strategy that we used in lecture to derive the ``slow'' rate 
  for the constrained lasso. Note that this was for fixed $X$, so you will need
  to condition on $X$ here. Then apply the result from Q2 part (b). You may use
  the fact that for $\epsilon = (\epsilon_1,\dots,\epsilon_n) \in \R^n$, a
  vector of i.i.d.\ sub-Gaussians with mean zero and variance proxy $\sigma^2$,
  and an arbitrary fixed vector $a \in \R^n$, the random variable $a^\T epsilon$
  is mean zero sub-Gaussian with variance proxy $\sigma^2 \|a\|_2^2$. 

\item For i.i.d.\ mean zero random variables $Z_i$, $i=1,\dots,n$ that lie
  almost surely in $[a,b]$, prove that for any $t \in \R$, 
  \marginpar{\small [2 pts]}
  \[
  \E \bigg[\exp \bigg(\frac{t}{n} \sum_{i=1}^n Z_i \bigg)\bigg] \leq
  e^{t^2(b-a)^2/(8n)}.  
  \]

  Hint: you may use the fact that each $Z_i$ is sub-Gaussian with variance proxy
  $(b-a)/2$, and the hint about linear combinations of sub-Gaussians from part
  (a).  

\item Let $\Sigma$ denote the predictor covariance matrix, that is,
  $\Sigma = \E[x_0x_0^\T]$ for a draw $x_0$ from the predictor distribution. Let
  \smash{$\hSigma = X^\T X/n$} be the empirical covariance matrix, and let
  \smash{$V = \hSigma-\Sigma$}. Prove that    
  \marginpar{\small [2 pts]}
  \[
  \E\Big[ \max_{j,k=1,\dots,d} \, |V_{jk}| \Big] \leq 2M^2 \sqrt{\frac{2
      \log(2d^2)}{n}}.  
  \]

  Hint: apply part (b) to the entries of $V$.

\item Prove that the lasso estimator, with $t=\|\beta_0\|_1$, has out-of-sample
  risk satisfying 
  \marginpar{\small [5 pts]}
  \[
  \E[(x_0^\T \hbeta - x_0^\T \beta_0)^2] \leq 4M \sigma \|\beta_0\|_1
  \sqrt{\frac{2 \log(2d)}{n}} + 8 M^2 \|\beta_0\|_1^2 \sqrt{\frac{2
      \log(2d^2)}{n}},  
  \]
  where the expectation is taken over the training data $(x_i,y_i)$,
  $i=1,\dots,n$ and an independent draw $x_0$ from the predictor distribution. 

  Hint: first, argue that the in-sample risk and out-of-sample risk can be
  written as 
  \[
  \E\big[ (\hbeta-\beta_0)^\T \hSigma (\hbeta-\beta_0) \big] 
  \quad \text{and} \quad
  \E\big[ (\hbeta-\beta_0)^\T \Sigma (\hbeta-\beta_0) \big],
  \]
  respectively. (Note that the expectations above are each taken with respect to
  the training samples $(x_i,y_i)$, $i=1,\dots,n$ only---there is nothing else
  that is random.) Next, argue that  
  \[
  (\hbeta-\beta_0)^\T \Sigma (\hbeta-\beta_0) -
  (\hbeta-\beta_0)^\T \hSigma (\hbeta-\beta_0) 
  \leq \sum_{j,k=1}^d (\hbeta-\beta_0)_j (\hbeta-\beta_0)_k |V_{jk}|,
  \]
  where recall \smash{$V_{jk} = (\hSigma-\Sigma)_{jk}$}. Then do a little bit
  more algebra to bound the right-hand side above and apply the previous parts
  of this question to conclude the result. 

\item The bound derived in this question for the out-of-sample risk is always
  larger than that for the in-sample risk (by nature of its construction). As a
  bonus, investigate: can the out-of-sample risk of the lasso be lower than the
  in-sample risk? Use a simulation, a pointer to an experiment or result in the
  literature, or any means of answering that you believe provides a convincing 
  argument.   
\end{enumerate}

\section{$\Gamma$-minimaxity of ridge regression [10 points]} 

\def\Risk{\mathrm{Risk}}

This exercise will explore a (strong, finite-sample) notion of minimax
optimality of ridge regression. Assume that $X \in \R^{n \times d}$ is a fixed,
arbitrary predictor matrix, and consider the following model:  
\begin{equation}
\label{eq:model}
\begin{gathered}
(\epsilon, \beta_0) \sim F^n \times G^d, \\
Y = X\beta_0 + \epsilon.
\end{gathered}
\end{equation}
Here, $F$ and $G$ are distributions on $\R$ that have mean zero and variance
$\sigma^2 > 0$ and $r^2 \geq 0$, respectively. Abbreviate $\gamma = (F,G)$, and 
let $\Gamma$ denote the set of all such pairs of distributions $(F,G)$ that
satisfy these moment conditions. 

We can think of $F$ and $G$ as describing a noise and prior distribution,
respectively: the components of $\epsilon$ are i.i.d.\ from $F$, and those of  
$\beta_0$ are i.i.d.\ from $G$. We measure risk according to:
\[
\Risk(\hbeta; \gamma) = \E \|\hbeta - \beta_0\|_2^2,
\]
where the expectation is over all that is random: $\epsilon, \beta_0, Y$ drawn 
from \eqref{eq:model}. To be specific, this is the \emph{Bayes} risk of an
estimator \smash{$\hbeta$}, though we'll often just call it risk for short. The
notation \smash{$\Risk(\hbeta; \gamma)$} emphasizes the dependence of the risk
on $\gamma$.    

\begin{enumerate}[label=(\alph*)]
\item Prove that the ridge regression estimator,
  \[
  \hbeta = \argmin_\beta \; \frac{1}{n} \|Y - X\beta\|_2^2 + \lambda
  \|\beta\|_2^2, 
  \]
  for any fixed tuning parameter value $\lambda > 0$, has risk 
  \marginpar{\small [4 pts]}
  \[
  \Risk(\hbeta; \gamma) = \frac{\sigma^2}{n} \tr \big[ \lambda^2 \alpha (\hSigma
  + \lambda I)^{-2} + \hSigma ( \hSigma + \lambda I)^{-2} \big],
  \]
  where \smash{$\hSigma = (X^\T X)/n$} and $\alpha = r^2n / (\sigma^2 d)$. Hint:
  use a bias-variance decomposition.

\item In general, consider estimation of a functional $\theta(P)$, given i.i.d.\ 
  draws from $P$, with respect to some metric $d$. Let $\Gamma$ let be class of 
  distributions $\gamma$, where each $\gamma = (P,\pi)$, with $P$ specifying the 
  distribution of the data, and $\pi$ specifying the distribution of the
  functional $\theta(P)$. We can think of $P$ as the likelihood and $\pi$ as the 
  prior. Then the $\Gamma$-minimax risk is defined as:    
  \[
  \inf_{\htheta} \, \sup_{\gamma \in \Gamma} \, \E_\gamma \big[ d(\theta(P),
  \htheta) \big]. 
  \]
  The expectation here is with respect to $\gamma$, and hence is a Bayes
  risk. Note that if for each $\gamma = (P,\pi)$, the prior is a point mass at a
  single value of $\theta(P)$, then $\Gamma$-minimax risk reduces to the usual
  notion of minimax risk defined in lecture.      

  Let \smash{$\htheta^B$} be the Bayes estimator with respect to some
  likelihood-prior pair $\gamma_0 = (P_0,\pi_0)$, where $\gamma_0 \in
  \Gamma$. Prove that if its Bayes risk is constant as we vary $\gamma \in
  \Gamma$, then it is $\Gamma$-minimax optimal,  
  \marginpar{\small [3 pts]}
  \[
  \sup_{\gamma \in \Gamma} \, \E_\gamma \big[ d(\theta(P), \htheta^B) \big]
  = \inf_{\htheta} \, \sup_{\gamma \in \Gamma} \, \E_\gamma \big[ d(\theta(P), 
  \htheta) \big].
  \]

  Hint: suppose not, and obtain a contradiction to the fact that
  \smash{$\htheta^B$} is Bayes. 

\item Returning to our regression problem setting, prove that ridge regression
  is the Bayes estimator with respect to a particular instantiation of $\gamma = 
  (F,G)$, and use the previous parts to establish that it is $\Gamma$-minimax
  for the class $\Gamma$ defined in part (a).  
  \marginpar{\small [3 pts]}

\item As a bonus: extend the previous result to the case of out-of-sample Bayes
  prediction risk. That is, instead of \eqref{eq:model}, consider
  \begin{equation}
  \label{eq:model_x0}
  \begin{gathered}
  (\epsilon, \beta_0,  x_0) \sim F^n \times G^d \times Q, \\
  Y = X\beta_0 + \epsilon,
  \end{gathered}
  \end{equation}
  where $F,G$ are as before, and now $Q$ is a distribution on $\R^d$ with mean
  zero and covariance $\Sigma \succ 0$. Abbreviate $\gamma = (F,G,Q)$, and let
  $\Gamma$ denote the set of all such triplets of distributions $(F,G,Q)$ that
  meet the specified moment conditions. While the training predictors $X$ are
  still fixed, $Q$ specifies the distribution of the test predictor $x_0$ used
  to measure risk, defined as: 
  \[
  \Risk(\hbeta; \gamma) = \E[ (x_0^\T \hbeta - x_0^\T \beta_0)^2 ], 
  \]
  where the expectation is over everything that is random: $\epsilon, \beta_0,
  x_0, Y$ drawn from \eqref{eq:model_x0}. For this model and new definition of
  risk, prove that ridge regression is still $\Gamma$-minimax optimal. Along the
  way, you will find it useful to prove that the out-of-sample Bayes prediction 
  risk of ridge is  
  \[
  \Risk(\hbeta; \gamma) = \frac{\sigma^2}{n} \tr \big[ \lambda^2 \alpha (\hSigma
  + \lambda I)^{-2} \Sigma + \hSigma ( \hSigma + \lambda I)^{-2} \Sigma \big].
  \]
\end{enumerate}

\bibliographystyle{plainnat}
\bibliography{../../common/ryantibs}

\end{document}